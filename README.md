Architecture Overview
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI       â”‚  â† Config Management API (CRUD)
â”‚   Config API    â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ stores/reads
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚  â† Campaign configurations
â”‚   Config DB     â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sample User    â”‚â”€â”€â”€â”€â–¶â”‚  Spark Predict   â”‚â”€â”€â”€â”€â–¶â”‚  Predictions    â”‚
â”‚  Features       â”‚     â”‚  Job (PySpark)   â”‚     â”‚  Output         â”‚
â”‚  (Parquet)      â”‚     â”‚  + CatBoost      â”‚     â”‚  (Parquet)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–²
                                â”‚ loads
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Model Artifact â”‚
                        â”‚ (.pkl file)    â”‚
                        â”‚  CatBoost      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
The Flow:

Config API: Marketer creates campaign config via FastAPI (which model, which audience)
Batch Job: Spark job reads config â†’ loads users â†’ loads model â†’ scores users
Results: Predictions written to parquet (in prod: BigQuery)

---

# Spark ML Pipeline - MLE Learning Project

A production-style ML pipeline demonstrating:
- **FastAPI** for campaign configuration CRUD
- **CatBoost** model training (tree-based classifier)
- **PySpark** for distributed batch prediction
- **PostgreSQL** for config storage
- **Docker** for local development
- **Kubernetes/DataProc** deployment patterns

## Prerequisites

1. **Python 3.9+** - `python3 --version`
2. **Docker Desktop** - For PostgreSQL container
3. **Java 11+** - Required by PySpark: `java -version`

## Quick Start (Local Development)

### 1. Install Dependencies

```bash
# Install Python packages
pip3 install -r requirements.txt
```

### 2. Start Infrastructure

```bash
# Start PostgreSQL with Docker
docker-compose up -d

# Verify it's running
docker ps
```

### 3. Train the Model

```bash
# Train CatBoost model and save to models/
python3 2_model_training/train_catboost_model.py
```

Expected output:
```
Model accuracy: 0.XXX
âœ… Model saved to models/campaign_model_v1.pkl
Required features: ['recency', 'frequency', 'monetary', 'engagement_score', 'days_since_last_purchase']
```

### 4. Generate Sample Data

```bash
# Create sample users for testing
python3 scripts/generate_sample_data.py
```

This creates `data/sample_users.parquet` with 1000 synthetic users.

### 5. Start the Config API

```bash
# Navigate to API directory
cd 1_config_api

# Run FastAPI with uvicorn
python3 -m uvicorn main:app --reload

# API will be available at http://localhost:8000
# Interactive docs at http://localhost:8000/docs
```

### 6. Create a Campaign (New Terminal)

```bash
# Create a campaign configuration
curl -X POST http://localhost:8000/campaigns/ \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Black Friday Campaign",
    "model_path": "models/campaign_model_v1.pkl",
    "audience_filter": "country='\''US'\'' AND age > 25",
    "features": ["recency", "frequency", "monetary", "engagement_score", "days_since_last_purchase"]
  }'
```

Response:
```json
{
  "id": 1,
  "name": "Black Friday Campaign",
  "model_path": "models/campaign_model_v1.pkl",
  "audience_filter": "country='US' AND age > 25",
  "features": [...],
  "is_active": true,
  "created_at": "2024-11-13T..."
}
```

### 7. Run Spark Prediction Job

```bash
# From project root
./3_predict_job/run_local.sh 1

# Or manually with spark-submit
spark-submit \
  --master local[*] \
  --driver-memory 2g \
  3_predict_job/predict_task.py \
  --campaign-id 1 \
  --config-api-url http://localhost:8000
```

Expected output:
```
ðŸš€ SPARK PREDICTION JOB STARTING
ðŸ“¡ Fetching campaign config from: http://localhost:8000/campaigns/1
âœ… Loaded campaign: Black Friday Campaign
ðŸ“¦ Loading model from: models/campaign_model_v1.pkl
ðŸ“Š Reading user data from: data/sample_users.parquet
âœ… Loaded 1,000 users
ðŸŽ¯ Applying audience filter: country='US' AND age > 25
âœ… Filtered to 312 users (31.2%)
ðŸ¤– Generating predictions...
ðŸ’¾ Writing predictions to: output/predictions_campaign_1.parquet
âœ… JOB COMPLETED SUCCESSFULLY
```

### 8. View Results

```bash
# Check output
ls -lh output/

# Read predictions with pandas
python3 -c "
import pandas as pd
df = pd.read_parquet('output/predictions_campaign_1.parquet')
print(df.head(10))
print(f'\nTotal predictions: {len(df)}')
print(f'Average score: {df.prediction_score.mean():.3f}')
"
```

## API Endpoints

Base URL: `http://localhost:8000`

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/campaigns/` | Create campaign |
| GET | `/campaigns/{id}` | Get campaign by ID |
| GET | `/campaigns/` | List all campaigns |
| PUT | `/campaigns/{id}` | Update campaign |
| DELETE | `/campaigns/{id}` | Delete campaign |
| GET | `/docs` | Interactive API docs |

## Project Structure

```
spark-ml-pipeline/
â”œâ”€â”€ 1_config_api/           # FastAPI application
â”‚   â”œâ”€â”€ main.py            # API routes (CRUD)
â”‚   â”œâ”€â”€ models.py          # Pydantic schemas
â”‚   â””â”€â”€ database.py        # SQLAlchemy ORM
â”œâ”€â”€ 2_model_training/       # Model training
â”‚   â””â”€â”€ train_catboost_model.py
â”œâ”€â”€ 3_predict_job/          # Spark prediction job
â”‚   â”œâ”€â”€ predict_task.py    # Main Spark job
â”‚   â”œâ”€â”€ data_processing.py # Ibis example
â”‚   â””â”€â”€ run_local.sh       # Wrapper script
â”œâ”€â”€ 4_kubernetes/           # Deployment configs
â”‚   â””â”€â”€ spark-job.yaml     # K8s/DataProc templates
â”œâ”€â”€ scripts/                # Utilities
â”‚   â””â”€â”€ generate_sample_data.py
â”œâ”€â”€ data/                   # User data (parquet)
â”œâ”€â”€ models/                 # Trained models (.pkl)
â”œâ”€â”€ output/                 # Predictions (parquet)
â”œâ”€â”€ requirements.txt
â””â”€â”€ docker-compose.yml
```

## Technologies Demonstrated

### Backend & API
- **FastAPI** - Modern async Python web framework
- **SQLAlchemy** - ORM for PostgreSQL
- **Pydantic** - Data validation and serialization

### Machine Learning
- **CatBoost** - Gradient boosting classifier
- **scikit-learn** - Model evaluation and data splitting
- **Pickle** - Model serialization pattern

### Data Processing
- **PySpark** - Distributed data processing
- **Ibis** - Portable dataframe library (see `data_processing.py`)
- **Pandas** - For Pandas UDFs in Spark

### Infrastructure
- **PostgreSQL** - Campaign config database
- **Docker Compose** - Local orchestration
- **Kubernetes** - Production deployment (see `4_kubernetes/`)

## Deployment

### Local Testing
```bash
docker-compose up -d
./scripts/demo.sh 
```

### Kubernetes
```bash
# Deploy with spark-on-k8s-operator
kubectl apply -f 4_kubernetes/spark-job.yaml
```

## Cleanup

```bash
# Stop PostgreSQL
docker-compose down

# Remove generated data
rm -rf data/*.parquet output/*.parquet models/*.pkl
```